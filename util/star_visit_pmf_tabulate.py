#!/usr/bin/env python
r"""
 star_visit_pmf_tabulate: make per-star table of detection visits

 Usage:
  star_visit_pmf_tabulate.py [-N n] SCENARIO

 where:
   SCENARIO: a scenario-summary directory (containing a "drm" subdirectory,
     but not naming the drm directory itself).
     In Sandbox convention, SCENARIO is typically in the sims/ hierarchy.

  Output files are placed in the directory
    SCENARIO/sched/
  At present, two files:
    detection-visits.html: a HTML table listing stars and detection visits
    detection-visits.csv: the CSV version of the above table, for later analysis

  Optionally:
    -N n => cap the number of DRMs examined to "n", to speed testing.

"""

import os
import sys
import pickle
import argparse
from collections import defaultdict
import pandas as pd
import numpy as np


############################################################
#
# Output methods
#
############################################################

def dump_vanilla(args, pmf_df, outfile):
    # HTML document open
    preamble = r'''<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <!-- autogenerated  -->
    <title>Visit PMF: %s</title>
    <link rel="stylesheet" href="/Local/www-resources/ensemble-reports.css">
    <link rel="shortcut icon" type="image/png" sizes="16x16" href="/Local/www-resources/favicon.png">
    <script src="/Local/www-resources/sorttable.js"></script>
<style>
    #pmf_table {
    border-collapse: collapse;
    }
    #pmf_table th, #pmf_table td {
    border: 1px solid #888;
    padding: 5px;
    text-align: left;
    }
</style>
</head>
'''

    # HTML body open
    body_begin = r'''<body>
    <h1>Star Visits: %s</h1>

    Upwards to <A href="../html/">parent scenario</A>

    <p>Ensemble: %s</p>
    <p>Size: %d sims</p>
    
    <h2>Detection Visit Report</h2>
    '''

    # dict of column formatters
    formatters = {}
    for col in pmf_df.columns:
        if pmf_df[col].dtype in ['float64', 'float32']:
            if col.startswith('P('):
                # P(Vn) columns
                formatters[col] = '{:.2g}'.format
            else:
                # other numerical columns
                formatters[col] = '{:.4g}'.format

    with open(outfile, "w") as f:
        f.write(preamble % (args.scenario_short, ))
        f.write(body_begin % (args.scenario_short, args.scenario_id, args.Nens))
        # f.write(f"<p>Scenario: {scenario_id}</p>\n")
        # f.write(styled.to_html())
        f.write(f"<p>Ndet is the number of detections, across the ensemble, for the named star. This is also the number of samples going in to the PMF columns P(Vn).\n</p>")
        f.write(
            pmf_df.to_html(
                formatters=formatters,
                index=False,
                classes='sortable',
                table_id='pmf_table'
                ))
        f.write("</body>\n</html>")


def dump_pandas(args, pmf_df, outfile):
    r'''Dump to a styled HTML table.'''

    # style for the html table
    style = [
        {'selector': 'table',
         'props': [('border-collapse', 'collapse')]},
        {'selector': 'th, td',
         'props': [('padding', '4px 8px'),
                   ('border', '1px solid #ddd')]},
        {'selector': 'th',
         'props': [
             ('background-color', '#646161'),
             ('color', 'white'),
             ('font-weight', 'bold'),
             ('text-align', 'center')
         ]}
    ]

    # build a nice styled dataframe for html
    styled = (pmf_df.style
        .set_table_styles(style)
        .hide(axis='index')
        .background_gradient(
            subset=['Ndet'], cmap='YlGn'
        )
        .background_gradient(
            subset=['Completeness'], cmap='Blues'
        )
        .background_gradient(
            subset=['Rank (C/t [1/day])'], cmap='Oranges'
        )
        .set_properties(**{'text-align': 'center'})
        .format({
            'Completeness': '{:.4f}',
            'Rank (C/t [1/day])': '{:.4f}'
        })
    )

    # save as standalone html file
    with open(outfile, "w") as f:
        f.write(
            "<html><head>\n<style>"
            "table {border-collapse:collapse;width:100%;} "
            "th,td {padding:4px 8px;border:1px solid #ddd;text-align:center;} "
            "th {background:#646161;color:white;}"
            "</style></head>\n<body>\n"
        )
        f.write(
            "<h3>First Detection Probabilities by Visit "
            "with Completeness and Rank</h3>\n"
        )
        f.write(f"<p>Scenario: {scenario_id}</p>\n")
        f.write(styled.to_html())
        f.write("</body>\n</html>")



############################################################
#
# Process data
#
############################################################

# TODO: various efficiency improvements are possible

# lookup a key by star name rather than sInd
def spc_lookup(key, spc, name):
    sInd1 = np.where(spc['Name'] == name)[0]
    val = spc[key][sInd1[0]]
    # unpack the units if present
    return getattr(val, 'value', val)


def get_drm(args, seed_file):
    '''Load files'''
    # DRM has form NNNN.pkl, SPC of form NNNN.spc (a bit hacky)
    fn_drm = os.path.join(args.drm_dir, seed_file)
    fn_spc = os.path.join(args.spc_dir, seed_file[:-4]+'.spc')

    # if un-paired, do not error out, but report it
    ok, drm, spc = False, {}, {}
    readable = True
    if not os.path.isfile(fn_drm):
        print(f'{args.progname}: Unreadable DRM {fn_drm}, skipping.', file=sys.stderr)
        readable = False
    if not os.path.isfile(fn_spc) and readable:
        print(f'{args.progname}: Unreadable SPC {fn_spc}, skipping.', file=sys.stderr)
        readable = False

    # but *do* error out if the DRM/SPC does not unpickle
    if readable:
        with open(fn_drm, 'rb') as fp_drm, open(fn_spc, 'rb') as fp_spc:
            drm = pickle.load(fp_drm)
            spc = pickle.load(fp_spc)
        ok = True

    return ok, drm, spc


def standardize(spc, entry):
    r'''tieredScheduler_DD DRMs sometimes do not have star_name in the DRM.
    (as of 2025/12)'''
    # in-place modification
    if 'star_name' not in entry:
        entry['star_name'] = spc['Name'][entry['star_ind']]


def process_drms(args):
    r'''Do all data processing for the entire ensemble of DRMs'''
    
    # counters and storage for detections and per-star completeness & rank
    first_detection_counts = defaultdict(int)
    star_first_detection_counts = defaultdict(lambda: defaultdict(int))
    total_star_detections = defaultdict(int)
    star_completeness = defaultdict(list)
    star_rank = defaultdict(list)

    ## 
    ## 1 -- Load files and reduce
    ## 

    # loop through each drm file in the drm folder
    Nens, Nskip = 0, 0
    for seed_file in os.listdir(args.drm_dir):
        ## 
        ## 1A -- Load files
        ## 
        # DRM must have form NNNN.pkl, ignore anything else
        if not seed_file.endswith('.pkl'):
            continue
        if args.Nmax > 0 and Nens >= args.Nmax:
            print(f'{args.progname}: Early stop at {Nens} DRM/SPC files.')
            break
        ok, drm, spc = get_drm(args, seed_file)

        if not ok:
            Nskip += 1
            continue # just skip
        else:
            Nens += 1

        ## 
        ## 1B -- Compute star-by-star statistics
        ## 
        # group observations by star name
        per_star_obs = defaultdict(list)
        for entry in drm:
            standardize(spc, entry)
            per_star_obs[entry['star_name']].append(entry)

        # process each starâ€™s observations
        for star_name, obs_list in per_star_obs.items():

            # check for the first detection (need a 1 in det_status)
            for idx, obs in enumerate(obs_list):
                det_status = obs.get('det_status', [])
                if (isinstance(det_status, (list, np.ndarray))
                        and 1 in det_status):
                    first_detection_counts[idx + 1] += 1
                    star_first_detection_counts[star_name][idx + 1] += 1
                    total_star_detections[star_name] += 1
                    break

            # get the completeness of this star from the SPC
            # (there's also an sInd in the obs)
            sInd = np.where(spc['Name'] == star_name)[0]
            # the ending [0] unboxes the length-1 array
            int_comp = spc['int_comp'][sInd][0]

            # Note: char_comp and char_time are phased out
            # look for the latest detection within the observation timeline
            # of a particular star and use it to compute completeness and rank
            last_det_obs = None
            for obs in reversed(obs_list):
                if ('det_comp' in obs and 'det_time' in obs
                        and obs['det_comp'] is not None
                        and obs['det_time'] is not None):
                    last_det_obs = obs
                    break
            if last_det_obs:
                det_comp = last_det_obs['det_comp']
                det_time = last_det_obs['det_time']
                completeness = int_comp
                # completeness = float(det_comp)
                # time_value = getattr(det_time, 'value', det_time) or 1
                time_value = getattr(det_time, 'value', det_time)
                rank = completeness / time_value
            else:
                completeness, rank = np.array(float('nan')), np.array(float('nan'))
            star_completeness[star_name].append(completeness)
            star_rank[star_name].append(rank)

    # Summarize
    if Nskip > 0:
        print(f'{args.progname}: Skipped {Nskip}, loaded {Nens} DRM/SPC files.')
    else:
        print(f'{args.progname}: Loaded {Nens} DRM/SPC files.')
    args.Nens = Nens # used later for the output files

    ## 
    ## 2 -- Average across ensemble
    ## 

    # compute averages across all stars
    avg_star_completeness = {
        k: np.nanmean(v) for k, v in star_completeness.items()
    }
    avg_star_rank = {
        k: np.nanmean(v) for k, v in star_rank.items()
    }

    # figure out the maximum visit number seen in order to scale the table
    # columns accordingly
    max_visits = max(
        (max(v.keys(), default=0) for v in star_first_detection_counts.values()),
        default=0
    )

    ## 
    ## 3 -- Make a data frame
    ## 

    # accumulate star-by-star summaries
    star_data = []
    for star_name, visit_counts in star_first_detection_counts.items():
        # Make a summary row for this star
        total_detects = total_star_detections[star_name]
        pmf_columns = {}
        tau = 0.0
        for visit in range(1, max_visits + 1):
            prob = visit_counts.get(visit, 0) / total_detects
            # drop the count if it is <= tau (is negligible)
            tau = 0.0
            pmf_columns[f'P(V{visit})'] = (
                f'{prob:.2f}' if prob > tau else ''
            )
        star_data.append({
            'Star': star_name,
            'Spec': spc_lookup('Spec', spc, star_name),
            'L': spc_lookup('L', spc, star_name),
            'Dist': spc_lookup('dist', spc, star_name),
            'Completeness': avg_star_completeness.get(star_name, np.nan),
            'Rank (C/t [1/day])': avg_star_rank.get(star_name, np.nan),
            'Ndet': total_detects,
            **pmf_columns
        })

    # make a dataframe, and sort by rank
    if len(star_data) > 0:
        pmf_df = pd.DataFrame(star_data).sort_values(
            'Rank (C/t [1/day])', ascending=False
        )
    else:
        pmf_df = pd.DataFrame([])

    return pmf_df


############################################################
#
# Dump results
#
############################################################

def dump(args, pmf_df):
    # make sure output directory exists
    os.makedirs(args.outdir, exist_ok=True)

    # use scenario folder name for output file label
    args.scenario_id = args.scenario # (in principle, different)
    args.scenario_short = os.path.basename(args.scenario)
    args.parent_id = os.path.basename(os.path.dirname(args.scenario))

    output_doc = os.path.join(args.outdir, 'detection-visits.html')
    output_csv = os.path.join(args.outdir, 'detection-visits.csv')

    if args.Nens == 0:
        print(f"{args.progname}: Warning: Output table will be empty.")

    # switch output formats
    # PANDAS_TABLE = True
    PANDAS_TABLE = False

    if PANDAS_TABLE:
        dump_pandas(args, pmf_df, output_doc)
    else:
        dump_vanilla(args, pmf_df, output_doc)
    print(f"{args.progname}: Table saved to {output_doc}")

    # dump also as csv -- convert the "no visits" entries to 0.0
    visit_columns = [col for col in pmf_df.columns if 'P(V' in col]

    # Replace empty strings with 0.0 in those columns
    pmf_df[visit_columns] = pmf_df[visit_columns].replace('', 0.0)

    # and write to CSV
    pmf_df.to_csv(output_csv, index=False)



############################################################
#
# main()
#
############################################################

def main(args):

    # required subdirectories
    args.drm_dir = os.path.join(args.scenario, 'drm')
    args.spc_dir = os.path.join(args.scenario, 'spc')

    # error-out early for bad inputs
    # (don't try to create output directory yet)
    for dir, name in (
        (args.scenario, 'Given scenario'),
        (args.drm_dir, 'DRM'),
        (args.spc_dir, 'SPC')):
        if not os.path.isdir(dir):
            print(f"{args.progname}: {name} directory `{dir}' does not exist.", file=sys.stderr)
            print(f"{args.progname}: Fatal. Exiting.", file=sys.stderr)
            return False

    df = process_drms(args)
    dump(args, df)
    return True


############################################################
#
# entry
#
############################################################

if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="Make per-star table of detection visits.",
        epilog='The input argument is a directory containing drm/ and spc/ sub-directories. For more on usage, see the top of the file.'
        )
    parser.add_argument('scenario', metavar='SCENARIO', help='single scenario directory name')
    parser.add_argument('-o', '--outdir', metavar='DIR', help='output directory name (default = SCENARIO/sched)', default='')
    parser.add_argument('-N', '--Nmax', type=int, default=0, help='Limit on DRM count (limits runtime to ease testing; default=ALL)')
    parser.add_argument('-v', help='verbosity', action='count', dest='verbose',
                            default=0)
    args = parser.parse_args()
    
    # set umask in hopes that files will be group-writable
    os.umask(0o002)

    VERBOSITY = args.verbose

    # program name, for convenience
    args.progname = os.path.basename(sys.argv[0])
    
    if not args.outdir:
        args.outdir = os.path.join(args.scenario, 'sched')
        
    # some starshade DRMs do not have star_name
    args.our_star_name = False

    # call the main
    ok = main(args)
    if ok:
        print(f'{args.progname}: Finished OK.')
    sys.exit(0 if ok else 2)


